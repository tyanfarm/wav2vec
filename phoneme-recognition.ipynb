{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f50ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\n",
      "------------------ -----------\n",
      "asttokens          3.0.0\n",
      "audioread          3.0.1\n",
      "certifi            2025.10.5\n",
      "cffi               2.0.0\n",
      "charset-normalizer 3.4.4\n",
      "colorama           0.4.6\n",
      "comm               0.2.3\n",
      "contourpy          1.3.2\n",
      "cycler             0.12.1\n",
      "debugpy            1.8.17\n",
      "decorator          5.2.1\n",
      "exceptiongroup     1.3.0\n",
      "executing          2.2.1\n",
      "fastdtw            0.3.4\n",
      "filelock           3.20.0\n",
      "fonttools          4.60.1\n",
      "fsspec             2025.9.0\n",
      "huggingface-hub    0.35.3\n",
      "idna               3.11\n",
      "ipykernel          7.0.0\n",
      "ipython            8.37.0\n",
      "jedi               0.19.2\n",
      "Jinja2             3.1.6\n",
      "joblib             1.5.2\n",
      "jupyter_client     8.6.3\n",
      "jupyter_core       5.8.1\n",
      "kiwisolver         1.4.9\n",
      "lazy_loader        0.4\n",
      "librosa            0.11.0\n",
      "llvmlite           0.45.1\n",
      "MarkupSafe         3.0.3\n",
      "matplotlib         3.10.7\n",
      "matplotlib-inline  0.1.7\n",
      "mpmath             1.3.0\n",
      "msgpack            1.1.2\n",
      "nest-asyncio       1.6.0\n",
      "networkx           3.4.2\n",
      "numba              0.62.1\n",
      "numpy              2.2.6\n",
      "packaging          25.0\n",
      "parso              0.8.5\n",
      "pillow             11.3.0\n",
      "pip                22.3.1\n",
      "platformdirs       4.5.0\n",
      "pooch              1.8.2\n",
      "prompt_toolkit     3.0.52\n",
      "psutil             7.1.0\n",
      "pure_eval          0.2.3\n",
      "pycparser          2.23\n",
      "Pygments           2.19.2\n",
      "pyparsing          3.2.5\n",
      "python-dateutil    2.9.0.post0\n",
      "pywin32            311\n",
      "PyYAML             6.0.3\n",
      "pyzmq              27.1.0\n",
      "regex              2025.9.18\n",
      "requests           2.32.5\n",
      "safetensors        0.6.2\n",
      "scikit-learn       1.7.2\n",
      "scipy              1.15.3\n",
      "setuptools         65.5.0\n",
      "six                1.17.0\n",
      "soundfile          0.13.1\n",
      "soxr               1.0.0\n",
      "stack-data         0.6.3\n",
      "sympy              1.14.0\n",
      "threadpoolctl      3.6.0\n",
      "tokenizers         0.22.1\n",
      "torch              2.8.0\n",
      "torchaudio         2.8.0\n",
      "torchvision        0.23.0\n",
      "tornado            6.5.2\n",
      "tqdm               4.67.1\n",
      "traitlets          5.14.3\n",
      "transformers       4.57.0\n",
      "typing_extensions  4.15.0\n",
      "urllib3            2.5.0\n",
      "wcwidth            0.2.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee14ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Tyan\\wav2vec\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from scipy.spatial.distance import cosine\n",
    "import librosa\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from typing import Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0797e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     import whisperx\n",
    "#     WHISPERX_AVAILABLE = True\n",
    "# except ImportError:\n",
    "#     WHISPERX_AVAILABLE = False\n",
    "#     print(\"WhisperX not available. Install with: pip install whisperx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979ba888",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WordAssessment:\n",
    "    word: str\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "    similarity_score: float\n",
    "    is_correct: bool\n",
    "    phonemes_reference: str\n",
    "    phonemes_learner: str\n",
    "    \n",
    "@dataclass\n",
    "class WordAlignment:\n",
    "    word: str\n",
    "    start: float\n",
    "    end: float\n",
    "    score: Optional[float] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee81c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForcedAligner:\n",
    "    \"\"\"Handles forced alignment to get exact word timings.\"\"\"\n",
    "    \n",
    "    def __init__(self, method=\"whisperx\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize forced aligner.\n",
    "        \n",
    "        Args:\n",
    "            method: \"whisperx\" \n",
    "            device: \"cuda\" or \"cpu\"\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.device = device\n",
    "        \n",
    "        if method == \"whisperx\":\n",
    "            # if not WHISPERX_AVAILABLE:\n",
    "            #     raise ImportError(\"WhisperX not installed. Run: pip install whisperx\")\n",
    "            # print(f\"Loading WhisperX model on {device}...\")\n",
    "            # self.asr_model = whisperx.load_model(\"base\", device, compute_type=\"float32\")\n",
    "            self.alignment_model = None\n",
    "            self.metadata = None \n",
    "            \n",
    "    def align_with_whisperx(self, audio_path: str, text: str) -> List[WordAlignment]:\n",
    "        \"\"\"Use WhisperX for precise word-level alignment.\"\"\"\n",
    "        # Load audio\n",
    "        audio = whisperx.load_audio(audio_path)\n",
    "        \n",
    "        # Transcribe\n",
    "        result = self.asr_model.transcribe(audio, batch_size=16)\n",
    "        \n",
    "        # Load alignment model if not loaded\n",
    "        if self.alignment_model is None:\n",
    "            self.alignment_model, self.metadata = whisperx.load_align_model(\n",
    "                language_code=\"en\", device=self.device\n",
    "            )\n",
    "        \n",
    "        # Align\n",
    "        result = whisperx.align(\n",
    "            result[\"segments\"], \n",
    "            self.alignment_model, \n",
    "            self.metadata, \n",
    "            audio, \n",
    "            self.device,\n",
    "            return_char_alignments=False\n",
    "        )\n",
    "        \n",
    "        # Extract word alignments\n",
    "        alignments = []\n",
    "        for segment in result[\"segments\"]:\n",
    "            if \"words\" in segment:\n",
    "                for word_info in segment[\"words\"]:\n",
    "                    alignments.append(WordAlignment(\n",
    "                        word=word_info[\"word\"].strip(),\n",
    "                        start=word_info[\"start\"],\n",
    "                        end=word_info[\"end\"],\n",
    "                        score=word_info.get(\"score\", 1.0)\n",
    "                    ))\n",
    "        \n",
    "        return alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4b88e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PronunciationAssessor:\n",
    "    def __init__(self, model_name=\"bookbot/wav2vec2-ljspeech-gruut\"):\n",
    "        \"\"\"Initialize the pronunciation assessor with wav2vec2 phoneme model.\"\"\"\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "        self.model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        self.aligner = ForcedAligner(method=\"whisperx\")\n",
    "        \n",
    "    def load_audio(self, audio_path: str, sr: int = 16000) -> np.ndarray:\n",
    "        \"\"\"Load audio file and resample to 16kHz.\"\"\"\n",
    "        audio, _ = librosa.load(audio_path, sr=sr)\n",
    "        return audio\n",
    "    \n",
    "    def get_phoneme_embeddings(self, audio: np.ndarray) -> Tuple[torch.Tensor, str]:\n",
    "        \"\"\"Extract phoneme-level embeddings and predicted phonemes from audio.\"\"\"\n",
    "        # Prepare input\n",
    "        inputs = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get hidden states (embeddings)\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[-1]  # Last layer\n",
    "            \n",
    "            # Get phoneme predictions\n",
    "            logits = outputs.logits\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)\n",
    "            phonemes = self.processor.batch_decode(predicted_ids)[0]\n",
    "            \n",
    "        return hidden_states.squeeze(0), phonemes\n",
    "    \n",
    "    # def align_words_to_audio(self, text: str, audio_duration: float) -> List[Tuple[str, float, float]]:\n",
    "    #     \"\"\"\n",
    "    #     Simple word alignment based on uniform distribution.\n",
    "    #     For production, use a forced alignment tool like Montreal Forced Aligner.\n",
    "    #     \"\"\"\n",
    "    #     words = text.split()\n",
    "    #     print(f\"List words: {words}\")\n",
    "    #     word_duration = audio_duration / len(words)\n",
    "    #     print(f\"Word duration: {word_duration}\")\n",
    "        \n",
    "    #     alignments = []\n",
    "    #     for i, word in enumerate(words):\n",
    "    #         start = i * word_duration\n",
    "    #         end = (i + 1) * word_duration\n",
    "    #         alignments.append((word, start, end))\n",
    "        \n",
    "    #     return alignments\n",
    "    \n",
    "    def extract_segment_embeddings(self, embeddings: torch.Tensor, \n",
    "                                   start_time: float, end_time: float, \n",
    "                                   audio_duration: float) -> torch.Tensor:\n",
    "        \"\"\"Extract embeddings for a specific time segment.\"\"\"\n",
    "        total_frames = embeddings.shape[0]\n",
    "        \n",
    "        # Convert time to frame indices\n",
    "        start_frame = int((start_time / audio_duration) * total_frames)\n",
    "        end_frame = int((end_time / audio_duration) * total_frames)\n",
    "        \n",
    "        # Ensure valid range\n",
    "        start_frame = max(0, start_frame)\n",
    "        end_frame = min(total_frames, end_frame)\n",
    "        \n",
    "        # Extract segment and average pool\n",
    "        segment_embeddings = embeddings[start_frame:end_frame]\n",
    "        \n",
    "        if segment_embeddings.shape[0] == 0:\n",
    "            return embeddings.mean(dim=0)\n",
    "        \n",
    "        return segment_embeddings.mean(dim=0)\n",
    "    \n",
    "    def compute_similarity(self, emb1: torch.Tensor, emb2: torch.Tensor) -> float:\n",
    "        \"\"\"Compute cosine similarity between two embeddings.\"\"\"\n",
    "        emb1_np = emb1.cpu().numpy()\n",
    "        emb2_np = emb2.cpu().numpy()\n",
    "        \n",
    "        # Cosine similarity (1 - cosine distance)\n",
    "        similarity = 1 - cosine(emb1_np, emb2_np)\n",
    "        return float(similarity)\n",
    "    \n",
    "    def assess_pronunciation(self, \n",
    "                           reference_audio_path: str,\n",
    "                           learner_audio_path: str,\n",
    "                           reference_text: str,\n",
    "                           similarity_threshold: float = 0.85) -> List[WordAssessment]:\n",
    "        \"\"\"\n",
    "        Compare reference and learner audio to identify pronunciation mistakes.\n",
    "        \n",
    "        Args:\n",
    "            reference_audio_path: Path to correct pronunciation audio\n",
    "            learner_audio_path: Path to learner's audio\n",
    "            reference_text: Transcript of the reference audio\n",
    "            similarity_threshold: Threshold for determining if pronunciation is correct\n",
    "            \n",
    "        Returns:\n",
    "            List of WordAssessment objects with detailed results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PRONUNCIATION ASSESSMENT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load audio files\n",
    "        print(\"\\n1. Loading audio files...\")\n",
    "        ref_audio = self.load_audio(reference_audio_path)\n",
    "        learner_audio = self.load_audio(learner_audio_path)\n",
    "        \n",
    "        ref_duration = len(ref_audio) / 16000\n",
    "        learner_duration = len(learner_audio) / 16000\n",
    "        \n",
    "        print(f\"   Reference duration: {ref_duration:.2f}s\")\n",
    "        print(f\"   Learner duration: {learner_duration:.2f}s\")\n",
    "        \n",
    "        # # Get exact word alignments using forced alignment\n",
    "        # print(\"\\n2. Performing forced alignment...\")\n",
    "        # ref_alignments = self.aligner.align_with_whisperx(reference_audio_path, reference_text)\n",
    "        # learner_alignments = self.aligner.align_with_whisperx(learner_audio_path, reference_text)\n",
    "        \n",
    "        # print(f\"   Found {len(ref_alignments)} words in reference\")\n",
    "        # print(f\"   Found {len(learner_alignments)} words in learner audio\")\n",
    "        # print(f\"Learner alignments: {learner_alignments}\")\n",
    "        \n",
    "        # # Display word timings\n",
    "        # print(\"\\n   Reference word timings:\")\n",
    "        # for align in ref_alignments:  # Show first 5\n",
    "        #     print(f\"     '{align.word}': {align.start:.2f}s - {align.end:.2f}s\")\n",
    "\n",
    "        # Get embeddings and phonemes\n",
    "        print(\"\\n3. Extracting phoneme embeddings...\")\n",
    "        ref_embeddings, ref_phonemes = self.get_phoneme_embeddings(ref_audio)\n",
    "        learner_embeddings, learner_phonemes = self.get_phoneme_embeddings(learner_audio)\n",
    "        \n",
    "        print(f\"   Reference phonemes: {ref_phonemes}\")\n",
    "        print(f\"   Learner phonemes: {learner_phonemes}\")\n",
    "        \n",
    "        # # Align words to audio\n",
    "        # print(\"\\n3. Aligning words to audio segments...\")\n",
    "        # word_alignments = self.align_words_to_audio(reference_text, ref_duration)\n",
    "        # learner_word_alignments = self.align_words_to_audio(reference_text, learner_duration)\n",
    "        \n",
    "        # Assess each word\n",
    "        print(\"\\n4. Assessing word-level pronunciation...\\n\")\n",
    "        assessments = []\n",
    "        \n",
    "        # # Match words from both alignments\n",
    "        # min_length = min(len(ref_alignments), len(learner_alignments))\n",
    "        \n",
    "        # # for (word, ref_start, ref_end), (_, learn_start, learn_end) in zip(\n",
    "        # #     word_alignments, learner_word_alignments\n",
    "        # # ):\n",
    "        # for i in range(min_length):\n",
    "        #     ref_align = ref_alignments[i]\n",
    "        #     learner_align = learner_alignments[i]\n",
    "            \n",
    "        #     # Extract word-level embeddings using exact timings\n",
    "        #     ref_word_emb = self.extract_segment_embeddings(\n",
    "        #         ref_embeddings, ref_align.start, ref_align.end, ref_duration\n",
    "        #     )\n",
    "        #     learner_word_emb = self.extract_segment_embeddings(\n",
    "        #         learner_embeddings, learner_align.start, learner_align.end, learner_duration\n",
    "        #     )\n",
    "            \n",
    "        #     # Compute similarity\n",
    "        #     similarity = self.compute_similarity(ref_word_emb, learner_word_emb)\n",
    "        #     is_correct = similarity >= similarity_threshold\n",
    "            \n",
    "        #     assessment = WordAssessment(\n",
    "        #         word=ref_align.word,\n",
    "        #         start_time=learner_align.start,\n",
    "        #         end_time=learner_align.end,\n",
    "        #         similarity_score=similarity,\n",
    "        #         is_correct=is_correct,\n",
    "        #         phonemes_reference=ref_phonemes,  # In production, extract per-word phonemes\n",
    "        #         phonemes_learner=learner_phonemes\n",
    "        #     )\n",
    "            \n",
    "        #     assessments.append(assessment)\n",
    "            \n",
    "        #    # Print result with exact timings\n",
    "        #     status = \"✓ CORRECT\" if is_correct else \"✗ INCORRECT\"\n",
    "        #     print(f\"   {ref_align.word:15s} | {learner_align.start:.2f}s-{learner_align.end:.2f}s | Score: {similarity:.3f} | {status}\")\n",
    "        \n",
    "        return assessments\n",
    "    \n",
    "    def print_summary(self, assessments: List[WordAssessment]):\n",
    "        \"\"\"Print a summary of the assessment results.\"\"\"\n",
    "        total_words = len(assessments)\n",
    "        correct_words = sum(1 for a in assessments if a.is_correct)\n",
    "        accuracy = (correct_words / total_words * 100) if total_words > 0 else 0\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total words: {total_words}\")\n",
    "        print(f\"Correct pronunciations: {correct_words}\")\n",
    "        print(f\"Incorrect pronunciations: {total_words - correct_words}\")\n",
    "        print(f\"Accuracy: {accuracy:.1f}%\")\n",
    "        \n",
    "        if total_words - correct_words > 0:\n",
    "            print(\"\\nWords that need practice:\")\n",
    "            for assessment in assessments:\n",
    "                if not assessment.is_correct:\n",
    "                    print(f\"  - {assessment.word} (score: {assessment.similarity_score:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f803012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 8.0-full_build-www.gyan.dev Copyright (c) 2000-2025 the FFmpeg developers\n",
      "built with gcc 15.2.0 (Rev8, Built by MSYS2 project)\n",
      "configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-lcms2 --enable-libxml2 --enable-gmp --enable-bzlib --enable-lzma --enable-libsnappy --enable-zlib --enable-librist --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-libbluray --enable-libcaca --enable-libdvdnav --enable-libdvdread --enable-sdl2 --enable-libaribb24 --enable-libaribcaption --enable-libdav1d --enable-libdavs2 --enable-libopenjpeg --enable-libquirc --enable-libuavs3d --enable-libxevd --enable-libzvbi --enable-liboapv --enable-libqrencode --enable-librav1e --enable-libsvtav1 --enable-libvvenc --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs2 --enable-libxeve --enable-libxvid --enable-libaom --enable-libjxl --enable-libvpx --enable-mediafoundation --enable-libass --enable-frei0r --enable-libfreetype --enable-libfribidi --enable-libharfbuzz --enable-liblensfun --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-dxva2 --enable-d3d11va --enable-d3d12va --enable-ffnvcodec --enable-libvpl --enable-nvdec --enable-nvenc --enable-vaapi --enable-libshaderc --enable-vulkan --enable-libplacebo --enable-opencl --enable-libcdio --enable-openal --enable-libgme --enable-libmodplug --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libshine --enable-libtheora --enable-libtwolame --enable-libvo-amrwbenc --enable-libcodec2 --enable-libilbc --enable-libgsm --enable-liblc3 --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-ladspa --enable-libbs2b --enable-libflite --enable-libmysofa --enable-librubberband --enable-libsoxr --enable-chromaprint --enable-whisper\n",
      "libavutil      60.  8.100 / 60.  8.100\n",
      "libavcodec     62. 11.100 / 62. 11.100\n",
      "libavformat    62.  3.100 / 62.  3.100\n",
      "libavdevice    62.  1.100 / 62.  1.100\n",
      "libavfilter    11.  4.100 / 11.  4.100\n",
      "libswscale      9.  1.100 /  9.  1.100\n",
      "libswresample   6.  1.100 /  6.  1.100\n",
      "\n",
      "Exiting with exit code 0\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fac1325c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 8.0-full_build-www.gyan.dev Copyright (c) 2000-2025 the FFmpeg developers\n",
      "built with gcc 15.2.0 (Rev8, Built by MSYS2 project)\n",
      "configuration: --enable-gpl --enable-version3 --enable-sta\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "ffmpeg_path = r'C:\\Users\\tyan\\AppData\\Local\\Microsoft\\WinGet\\Packages\\Gyan.FFmpeg_Microsoft.Winget.Source_8wekyb3d8bbwe\\ffmpeg-8.0-full_build\\bin'\n",
    "os.environ['PATH'] = ffmpeg_path + os.pathsep + os.environ.get('PATH', '')\n",
    "# Now run your test\n",
    "import subprocess\n",
    "result = subprocess.run(['ffmpeg', '-version'], capture_output=True, text=True)\n",
    "print(result.stdout[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "620057e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: bookbot/wav2vec2-ljspeech-gruut\n",
      "\n",
      "============================================================\n",
      "PRONUNCIATION ASSESSMENT\n",
      "============================================================\n",
      "\n",
      "1. Loading audio files...\n",
      "   Reference duration: 1.05s\n",
      "   Learner duration: 1.00s\n",
      "\n",
      "3. Extracting phoneme embeddings...\n",
      "   Reference phonemes: f ɛ b j u ɛ ɹ i\n",
      "   Learner phonemes: d͡ʒæ n j u ɛ ɹ i\n",
      "\n",
      "4. Assessing word-level pronunciation...\n",
      "\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Total words: 0\n",
      "Correct pronunciations: 0\n",
      "Incorrect pronunciations: 0\n",
      "Accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Initialize assessor\n",
    "assessor = PronunciationAssessor()\n",
    "\n",
    "# Define your audio files and reference text\n",
    "reference_audio = \"./audio_files/word_february.mp3\"  # Correct pronunciation\n",
    "learner_audio = \"./audio_files/word_january.mp3\"      # Audio to be assessed\n",
    "reference_text = \"february\"  # Transcript\n",
    "\n",
    "# Perform assessment\n",
    "assessments = assessor.assess_pronunciation(\n",
    "    reference_audio_path=reference_audio,\n",
    "    learner_audio_path=learner_audio,\n",
    "    reference_text=reference_text,\n",
    "    similarity_threshold=0.90  # Adjust based on your needs (0.0-1.0)\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "assessor.print_summary(assessments)\n",
    "\n",
    "# # Access individual word results\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"DETAILED RESULTS\")\n",
    "# print(\"=\"*60)\n",
    "# for assessment in assessments:\n",
    "#     print(f\"\\nWord: {assessment.word}\")\n",
    "#     print(f\"  Time: {assessment.start_time:.2f}s - {assessment.end_time:.2f}s\")\n",
    "#     print(f\"  Similarity: {assessment.similarity_score:.3f}\")\n",
    "#     print(f\"  Status: {'Correct' if assessment.is_correct else 'Needs improvement'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a30564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Tyan\\wav2vec\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2FeatureExtractor\n",
    "from scipy.spatial.distance import cosine\n",
    "import librosa\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3a591c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 5080\n",
      "(12, 0)\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.cuda.get_device_capability())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332e93f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5f2df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path, sr=16000):\n",
    "    y, _ = librosa.load(path, sr=sr, mono=True)\n",
    "    # chuẩn hóa biên độ để ổn định so sánh\n",
    "    if np.max(np.abs(y)) > 0:\n",
    "        y = y / np.max(np.abs(y))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38cc272",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Received a bool for argument tokenizer, but a PreTrainedTokenizerBase was expected.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load model and processor\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mWav2Vec2Processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/wav2vec2-lv-60-espeak-cv-ft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Wav2Vec2ForCTC\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/wav2vec2-lv-60-espeak-cv-ft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m ref_audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./audio_files/word_february.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Tyan\\wav2vec\\myenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:59\u001b[0m, in \u001b[0;36mWav2Vec2Processor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     61\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     62\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a tokenizer inside \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from a config that does not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m include a `tokenizer_class` attribute is deprecated and will be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Tyan\\wav2vec\\myenv\\lib\\site-packages\\transformers\\processing_utils.py:1396\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m   1394\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1395\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_args_and_dict(args, processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Tyan\\wav2vec\\myenv\\lib\\site-packages\\transformers\\processing_utils.py:1197\u001b[0m, in \u001b[0;36mProcessorMixin.from_args_and_dict\u001b[1;34m(cls, args, processor_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1194\u001b[0m args \u001b[38;5;241m=\u001b[39m [args_to_update\u001b[38;5;241m.\u001b[39mget(i, arg) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(args)]\n\u001b[0;32m   1196\u001b[0m \u001b[38;5;66;03m# instantiate processor with used (and valid) kwargs only\u001b[39;00m\n\u001b[1;32m-> 1197\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvalid_kwargs)\n\u001b[0;32m   1199\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_unused_kwargs:\n",
      "File \u001b[1;32mc:\\Tyan\\wav2vec\\myenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:52\u001b[0m, in \u001b[0;36mWav2Vec2Processor.__init__\u001b[1;34m(self, feature_extractor, tokenizer)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, feature_extractor, tokenizer):\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_processor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Tyan\\wav2vec\\myenv\\lib\\site-packages\\transformers\\processing_utils.py:534\u001b[0m, in \u001b[0;36mProcessorMixin.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Check each arg is of the proper class (this will also catch a user initializing in the wrong order)\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attribute_name, arg \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 534\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_argument_for_proper_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattribute_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attribute_name, arg)\n",
      "File \u001b[1;32mc:\\Tyan\\wav2vec\\myenv\\lib\\site-packages\\transformers\\processing_utils.py:614\u001b[0m, in \u001b[0;36mProcessorMixin.check_argument_for_proper_class\u001b[1;34m(self, argument_name, argument)\u001b[0m\n\u001b[0;32m    611\u001b[0m     proper_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_possibly_dynamic_module(class_name)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(argument, proper_class):\n\u001b[1;32m--> 614\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    615\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(argument)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margument_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was expected.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    616\u001b[0m     )\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proper_class\n",
      "\u001b[1;31mTypeError\u001b[0m: Received a bool for argument tokenizer, but a PreTrainedTokenizerBase was expected."
     ]
    }
   ],
   "source": [
    "# load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "\n",
    "ref_audio_path = \"./audio_files/word_february.mp3\"\n",
    "\n",
    "ref_audio = load_audio(ref_audio_path)\n",
    "\n",
    "# tokenize\n",
    "input_values = processor(ref_audio, return_tensors=\"pt\").input_values\n",
    "\n",
    "# retrieve logits\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "# => should give ['m ɪ s t ɚ k w ɪ l t ɚ ɹ ɪ z ð ɪ ɐ p ɑː s əl ʌ v ð ə m ɪ d əl k l æ s ᵻ z æ n d w iː ɑːɹ ɡ l æ d t ə w ɛ l k ə m h ɪ z ɡ ɑː s p əl']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_phoneme_embeddings(audio: np.ndarray) -> Tuple[torch.Tensor, str]:\n",
    "#     \"\"\"Extract phoneme-level embeddings and predicted phonemes from audio.\"\"\"\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "#     processor = Wav2Vec2Processor.from_pretrained(\"bookbot/wav2vec2-ljspeech-gruut\")\n",
    "#     model = Wav2Vec2ForCTC.from_pretrained(\"bookbot/wav2vec2-ljspeech-gruut\")\n",
    "#     model.to(device)  # Di chuyển model lên GPU\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Prepare input và di chuyển lên GPU\n",
    "#     inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}  # Chuyển tất cả inputs lên GPU\n",
    "    \n",
    "#     with torch.inference_mode():  # Tối ưu hơn no_grad cho inference\n",
    "#         # Get hidden states (embeddings)\n",
    "#         outputs = model(**inputs, output_hidden_states=True)\n",
    "#         hidden_states = outputs.hidden_states[-1]  # Last layer, vẫn trên GPU\n",
    "        \n",
    "#         # Get phoneme predictions\n",
    "#         logits = outputs.logits\n",
    "#         predicted_ids = torch.argmax(logits, dim=-1)\n",
    "#         # predicted_ids_cpu = predicted_ids.cpu()  # Chuyển về CPU cho decode\n",
    "#         # phonemes = processor.batch_decode(predicted_ids_cpu)[0]\n",
    "\n",
    "#     return hidden_states.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f79e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"bookbot/wav2vec2-ljspeech-gruut\")\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"bookbot/wav2vec2-ljspeech-gruut\").to(device).eval()\n",
    "\n",
    "# def get_phoneme_embeddings_fast(audio: np.ndarray):\n",
    "#     inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=False)\n",
    "#     input_values = inputs.input_values.to(device, non_blocking=True)\n",
    "#     attention_mask = inputs.get(\"attention_mask\")\n",
    "#     if attention_mask is not None:\n",
    "#         attention_mask = attention_mask.to(device, non_blocking=True)\n",
    "\n",
    "#     with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=torch.cuda.is_available()), \\\n",
    "#          torch.inference_mode():\n",
    "#         out = model(input_values, attention_mask=attention_mask)  # không xin hidden_states\n",
    "#         last = out.last_hidden_state.squeeze(0)                   # [T, H]\n",
    "#         ids = torch.argmax(out.logits, dim=-1).cpu().numpy()      # decode trên CPU\n",
    "#     phonemes = processor.batch_decode(ids)[0]\n",
    "#     return last, phonemes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "56a30564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from scipy.spatial.distance import cosine\n",
    "import librosa\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b3a591c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 5080\n",
      "(12, 0)\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.cuda.get_device_capability())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "332e93f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e38cc272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(audio_path: str, sr: int = 16000) -> np.ndarray:\n",
    "    \"\"\"Load audio file and resample to 16kHz.\"\"\"\n",
    "    audio, _ = librosa.load(audio_path, sr=sr)\n",
    "    return audio\n",
    "\n",
    "def get_phoneme_embeddings(audio: np.ndarray) -> Tuple[torch.Tensor, str]:\n",
    "    \"\"\"Extract phoneme-level embeddings and predicted phonemes from audio.\"\"\"\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"bookbot/wav2vec2-ljspeech-gruut\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\"bookbot/wav2vec2-ljspeech-gruut\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get hidden states (embeddings)\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Last layer\n",
    "        \n",
    "        # Get phoneme predictions\n",
    "        logits = outputs.logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        phonemes = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "    return hidden_states.squeeze(0), phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8ebd7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_phoneme_embeddings(\n",
    "#     audio: np.ndarray,\n",
    "# ) -> Tuple[torch.Tensor, str]:\n",
    "#     \"\"\"\n",
    "#     Extract phoneme-level embeddings and predicted phonemes from audio.\n",
    "#     - Trả về: (embeddings cuối layer [T, H], chuỗi phoneme decode)\n",
    "#     - embeddings ở trên 'device'; nếu muốn đưa về CPU: .cpu() ở chỗ gọi\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Tải processor/model\n",
    "#     processor = Wav2Vec2Processor.from_pretrained(\"bookbot/wav2vec2-ljspeech-gruut\")\n",
    "#     model = Wav2Vec2ForCTC.from_pretrained(\"bookbot/wav2vec2-ljspeech-gruut\").to(device)\n",
    "#     model.eval()\n",
    "\n",
    "#     # Chuẩn bị input\n",
    "#     inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "#     input_values = inputs.input_values.to(device)\n",
    "#     attention_mask = inputs.get(\"attention_mask\")\n",
    "#     if attention_mask is not None:\n",
    "#         attention_mask = attention_mask.to(device)\n",
    "\n",
    "#     with torch.inference_mode():\n",
    "#         outputs = model(input_values, attention_mask=attention_mask, output_hidden_states=True)\n",
    "\n",
    "#         # Embeddings (layer cuối) -> [B, T, H] -> [T, H]\n",
    "#         hidden_states = outputs.hidden_states[-1].squeeze(0)\n",
    "\n",
    "#         # Dự đoán phoneme IDs và decode (đưa ids về CPU cho batch_decode)\n",
    "#         logits = outputs.logits\n",
    "#         predicted_ids = torch.argmax(logits, dim=-1)           # [B, T_ids]\n",
    "#         phonemes = processor.batch_decode(predicted_ids.cpu().numpy())[0]\n",
    "\n",
    "#     return hidden_states, phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9f79e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"bookbot/wav2vec2-ljspeech-gruut\")\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"bookbot/wav2vec2-ljspeech-gruut\").to(device).eval()\n",
    "\n",
    "# def get_phoneme_embeddings_fast(audio: np.ndarray):\n",
    "#     inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=False)\n",
    "#     input_values = inputs.input_values.to(device, non_blocking=True)\n",
    "#     attention_mask = inputs.get(\"attention_mask\")\n",
    "#     if attention_mask is not None:\n",
    "#         attention_mask = attention_mask.to(device, non_blocking=True)\n",
    "\n",
    "#     with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=torch.cuda.is_available()), \\\n",
    "#          torch.inference_mode():\n",
    "#         out = model(input_values, attention_mask=attention_mask)  # không xin hidden_states\n",
    "#         last = out.last_hidden_state.squeeze(0)                   # [T, H]\n",
    "#         ids = torch.argmax(out.logits, dim=-1).cpu().numpy()      # decode trên CPU\n",
    "#     phonemes = processor.batch_decode(ids)[0]\n",
    "#     return last, phonemes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "46cfe276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d͡ʒæ n j u ɛ ɹ i'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_audio_path = \"./audio_files/word_january.mp3\"\n",
    "\n",
    "ref_audio = load_audio(ref_audio_path)\n",
    "ref_embeddings, ref_phonemes = get_phoneme_embeddings(ref_audio)\n",
    "\n",
    "ref_phonemes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

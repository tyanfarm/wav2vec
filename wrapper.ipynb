{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f516982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Tyan\\wav2vec\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from scipy.spatial.distance import cosine\n",
    "import librosa\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254bc602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhonemeExtractor:\n",
    "    \"\"\"Class để load model một lần và tái sử dụng\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"bookbot/wav2vec2-ljspeech-gruut\", device: str = None):\n",
    "        \"\"\"\n",
    "        Initialize model and processor once\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name\n",
    "            device: 'cuda' or 'cpu', auto-detect if None\n",
    "        \"\"\"\n",
    "        print(f\"Loading model {model_name}...\")\n",
    "        \n",
    "        # Auto-detect device\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        # Load processor và model một lần\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "        self.model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"Model loaded on {self.device}\")\n",
    "    \n",
    "    def load_audio(self, audio_path: str, sr: int = 16000) -> np.ndarray:\n",
    "        \"\"\"Load audio file and resample to 16kHz.\"\"\"\n",
    "        audio, _ = librosa.load(audio_path, sr=sr)\n",
    "        return audio\n",
    "    \n",
    "    def get_phoneme_embeddings(self, audio: np.ndarray) -> Tuple[torch.Tensor, str]:\n",
    "        \"\"\"Extract phoneme-level embeddings and predicted phonemes from audio.\"\"\"\n",
    "        # Prepare input\n",
    "        inputs = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get hidden states (embeddings)\n",
    "            outputs = self.model(**inputs, output_hidden_states=False)\n",
    "            # hidden_states = outputs.hidden_states[-1]  # Last layer\n",
    "            \n",
    "            # Get phoneme predictions\n",
    "            logits = outputs.logits\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)\n",
    "            print(f\"Predicted IDs: {predicted_ids}\")\n",
    "            phonemes = self.processor.batch_decode(predicted_ids)[0]\n",
    "        torch.cuda.empty_cache()\n",
    "        return phonemes\n",
    "    \n",
    "    def process_file(self, audio_path: str) -> Tuple[torch.Tensor, str]:\n",
    "        \"\"\"Load audio và extract phonemes trong một lần gọi\"\"\"\n",
    "        audio = self.load_audio(audio_path)\n",
    "        return self.get_phoneme_embeddings(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1187f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model bookbot/wav2vec2-ljspeech-gruut...\n",
      "Model loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Load model một lần duy nhất (chỉ chạy 1 lần)\n",
    "extractor = PhonemeExtractor(device='cuda')  # hoặc 'cuda' nếu có GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce08d51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted IDs: tensor([[16,  0, 35, 35,  0,  0, 34,  0,  0, 14,  0,  0, 18, 18,  0,  0, 34, 34,\n",
      "          0,  0,  0,  0,  0, 26, 26,  0,  0,  0,  0,  0, 42, 42, 42,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0, 34, 34,  0,  0, 14,  0,  0, 25,  0,  0,\n",
      "          0,  0, 30, 30, 15, 15, 14,  0,  0,  0,  0,  0,  0,  0, 12,  0,  0,  9,\n",
      "          9,  0,  0,  0,  0,  0,  0, 17, 17,  0,  0, 32,  0,  0,  0,  0, 14, 14,\n",
      "          0,  0,  0,  0, 17,  0,  0,  0,  0, 22,  0, 34,  0,  0,  0, 40,  0,  0,\n",
      "         22, 22,  0,  0,  0,  0,  0, 34, 19, 19, 22, 22,  0,  0,  0,  0,  9,  0,\n",
      "          0,  0,  0, 27,  0,  0,  0, 35, 35,  0,  0,  0, 24,  0,  0, 18,  0,  0,\n",
      "          0, 16,  0,  0, 35,  0, 32, 32,  0,  0, 23,  0, 30, 30,  0, 14,  0,  0,\n",
      "         18,  0,  0, 11,  0,  0, 30,  0, 14, 14,  0,  0,  0,  0, 17, 17,  0,  0,\n",
      "          0, 31, 31,  0,  0,  0,  0,  0,  0,  0,  0, 14, 14,  0,  0,  0,  4,  0,\n",
      "          0, 42,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  4,  0,  0, 34,  0,  0,  0,  7,  7,  0,  0,  0,\n",
      "         31, 31,  0,  0,  0,  0,  0,  0, 23, 23,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          7,  7,  0,  0, 35,  0, 38,  0,  0, 13,  0,  0,  0, 13, 13, 15, 15, 17,\n",
      "         17,  0,  0,  0,  0,  0,  0,  0,  0,  0, 18,  0,  0,  0, 34,  0,  0,  7,\n",
      "          7,  0,  0,  0,  0, 14,  0,  0, 27,  0,  0,  0,  0, 18,  0,  0,  0,  7,\n",
      "          0,  0, 35,  0,  0, 38,  0, 13, 13, 13,  0,  0,  0,  0,  0,  0, 28, 28,\n",
      "          0,  0,  0,  0, 12, 12,  0,  0,  0,  0, 25,  0, 30, 30,  0,  0,  0, 27,\n",
      "         27,  0, 35, 35,  0,  0,  0, 18,  0,  0, 17,  0,  0, 24,  0, 14, 14,  0,\n",
      "          4,  0,  0, 11,  0,  0, 35, 35,  0,  0, 24,  0,  0,  0,  0,  0,  0,  7,\n",
      "          7,  0,  0,  0, 18,  0,  0, 17, 17,  0,  0,  0,  0, 35, 35,  0,  0, 32,\n",
      "          0,  0, 16, 16,  0, 35,  0,  0, 34,  0, 23, 23,  0,  0, 32,  0, 14, 14,\n",
      "          0, 18,  0,  0, 34,  0,  0,  4,  4,  0,  0,  0,  0,  0, 34,  0,  0, 14,\n",
      "          0,  0, 25,  0,  0, 30,  0,  0,  0, 32,  0,  0,  0, 11, 11,  0, 17, 17,\n",
      "          0,  0,  0, 34,  0,  0,  3,  3,  0,  0, 34,  0,  0,  0,  0, 36,  0,  0,\n",
      "         30,  0,  0,  0, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "       device='cuda:0')\n",
      "p ɹ ɪ n t ɪ ŋ  ɪ n ð əoʊn l i s ɛ n s w ɪ θ w ɪt͡ʃw i ɑ ɹ æ t p ɹ ɛ z ə n t k ə n s ɚ n d  d ɪ f ɚ z f ɹ ʌ m moʊs t ɪ f n ɑ t f ɹ ʌ m ɔ l ð ə ɑ ɹ t s æ n d k ɹ æ f t s ɹ ɛ p ɹ ɪ z ɛ n t ɪ d ɪ n ð ə ɛ k s ɪ b ɪ ʃ ə n\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Sử dụng nhiều lần mà không cần load lại\n",
    "ref_phonemes = extractor.process_file(\"./audio_files/lj_01.wav\")\n",
    "print(ref_phonemes)\n",
    "# print(ref_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a32c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
